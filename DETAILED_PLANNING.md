# **Detailed Planning - Music Emotion Annotation Tool**

## **Granulare Task-Liste f√ºr ausf√ºhrbare Entwicklung**

---

## **üìã Aktueller Stand (v0.2.0) ‚úÖ ABGESCHLOSSEN**

### **Phase 1: MVP & Grundlagen**

- ‚úÖ Core Audio-Funktionalit√§t (Web Audio API)
- ‚úÖ Basis-UI mit Tailwind/shadcn
- ‚úÖ KI-Integration (Google Gemini)
- ‚úÖ Marker-Erstellung und -Verwaltung
- ‚úÖ Code-Konsolidierung (25% Reduktion)
- ‚úÖ Bundle-Optimierung (Vendor-Chunks)
- ‚úÖ Dokumentations-Konsolidierung

### **Phase 1.5: Bugfixes & Stabilit√§t ‚úÖ ABGESCHLOSSEN**

- ‚úÖ Timeline-Steuerung (Click & Drag Panning)
- ‚úÖ Zoom-Funktionalit√§t (Passive Event Listener Fix)
- ‚úÖ AudioContext-Initialisierung (User Gesture Requirement)
- ‚úÖ Hotspot-Speicherung (IndexedDB Integration)
- ‚úÖ Hover-Funktionalit√§t (2D Hit-Test f√ºr Tooltips)
- ‚úÖ AI-Analyse-Performance (Doppelte Aufrufe eliminiert)
- ‚úÖ AI-Fehlerbehandlung & Marker-Erhaltung (Debug-Logs bereinigt)

### **Technische Basis**

- ‚úÖ React 19 + TypeScript + Vite
- ‚úÖ TailwindCSS + shadcn/ui
- ‚úÖ TensorFlow.js Integration
- ‚úÖ Lokale Datenspeicherung
- ‚úÖ Responsive Design (Grundlagen)

---

## **üéØ Phase 2: Forschungsvalidierung & App-Verbesserung (AKTUELL)**

### **Aktueller Entwicklungsstand**

**‚úÖ Abgeschlossene Tasks:**

- **T-001**: Mehrspurige Timeline (5 Tracks implementiert)
- **T-002**: Emotionale Hotspot-Visualisierung ‚úÖ **VOLLST√ÑNDIG ABGESCHLOSSEN**
- **T-003.1**: Erweiterte Track-Informationen im Tooltip ‚úÖ **ABGESCHLOSSEN**
- **T-008**: Audio-Caching (IndexedDB-basiert)
- **T-018**: Onset Detection (Phrase-Grenzen, Beat-Tracking)
- **T-019**: Erweiterte Audio-Features (Vocal-Presence, Dynamic-Intensity, Harmonic-Complexity)
- **T-020**: Audio-Feature-Integration (Text-Sync-Priority-Track)

**üîÑ Aktuell in Bearbeitung:**

- **T-003.2**: Erweiterte KI-Analyse mit musikalischem Kontext (n√§chster Schritt)

**üìã N√§chste Priorit√§ten:**

1. **T-003**: Mini-Spektrogramm hinzuf√ºgen
2. **T-006**: Lazy Loading f√ºr gro√üe Audiodateien
3. **T-011**: Touch-Gesten f√ºr Timeline

### **2.1 Erweiterte Visualisierungen (Priorit√§t: HOCH)**

#### **Task 2.1.1: Timeline-Visualisierung verbessern**

- [x] **T-001**: Mehrspurige Timeline implementieren ‚úÖ

  - **Beschreibung**: Separate Spuren f√ºr Amplitude, Spectral Features, KI-Hotspots
  - **Abh√§ngigkeiten**: Timeline.tsx refactoren
  - **Akzeptanzkriterien**: 3 Spuren sichtbar, Performance < 100ms
  - **Status**: ‚úÖ ABGESCHLOSSEN - Mehrspurige Timeline mit 5 Tracks implementiert

- [x] **T-002**: Emotionale Hotspot-Visualisierung ‚úÖ **VOLLST√ÑNDIG ABGESCHLOSSEN**

  - **Beschreibung**: Emoji-basierte Marker mit farbigen Kreisen und Tooltips
  - **Abh√§ngigkeiten**: GEMS_COLORS erweitern
  - **Akzeptanzkriterien**: 5 verschiedene Emotionstypen unterscheidbar
  - **Status**: ‚úÖ ABGESCHLOSSEN - Emoji-Marker, Tooltip-Positionierung, Scroll-Offset-Fix, Container-Synchronisation

  - [x] **T-002.1**: Visuelle Hierarchie & Emotion Mapping ‚úÖ

    - **Beschreibung**: Tooltip-Header in GEMS-Farben, Valence/Arousal-Indikatoren
    - **Abh√§ngigkeiten**: T-002 (aktuelle einheitliche Marker)
    - **Akzeptanzkriterien**: Jede Emotion hat ihre charakteristische Farbe im Tooltip
    - **Status**: ‚úÖ ABGESCHLOSSEN - GEMS-Farben, Valence/Arousal-Indikatoren, verbesserte Tooltip-Layouts implementiert

  - [x] **T-002.2**: Interaktive Datenvisualisierung ‚úÖ

    - **Beschreibung**: Mini-Balkendiagramme f√ºr Intensit√§t, Confidence-Ringe, GEMS-Pie-Charts
    - **Abh√§ngigkeiten**: T-002.1
    - **Akzeptanzkriterien**: Visuelle Darstellung aller numerischen Werte
    - **Status**: ‚úÖ ABGESCHLOSSEN - Mini-Balkendiagramme, Confidence-Ringe um Rauten, GEMS-Pie-Charts im Tooltip implementiert + UX-Verbesserungen (subtilere Ringe, Emotion-Icons, vereinfachte Visualisierungen)

  - [x] **T-002.3**: Emotionale Kontextualisierung ‚úÖ

    - **Beschreibung**: Emoji-basierte Marker mit emotionaler Bedeutung
    - **Abh√§ngigkeiten**: T-002.2
    - **Akzeptanzkriterien**: Emotionen sind emotional verst√§ndlich
    - **Status**: ‚úÖ ABGESCHLOSSEN - Emoji-Marker ersetzen Rauten, emotionale Kontextualisierung durch visuelle Emoji-Darstellung

  - [x] **T-002.4**: Musikalische Kontextualisierung ‚úÖ
    - **Beschreibung**: Tooltip-Positionierung, Scroll-Offset-Handling, Container-Synchronisation
    - **Abh√§ngigkeiten**: T-002.3
    - **Akzeptanzkriterien**: Vollst√§ndiger musikalischer Kontext verf√ºgbar
    - **Status**: ‚úÖ ABGESCHLOSSEN - Tooltip-Positionierung mit Scroll-Offset, Container-H√∂hen-Synchronisation, vertikaler Scrollbar-Fix

- [ ] **T-003**: Tooltip-Erweiterungen f√ºr bessere Datenkontextualisierung

  - **Beschreibung**: Erweiterte Track-Informationen, KI-Analyse und Audio-Feature-Korrelation im Tooltip
  - **Abh√§ngigkeiten**: Timeline.tsx Tooltip, bestehende Audio-Features
  - **Akzeptanzkriterien**: Alle Audio-Features werden im Tooltip angezeigt, KI-Insights sind erweitert

  - [x] **T-003.1**: Erweiterte Track-Informationen im Tooltip ‚úÖ

    - **Beschreibung**: Aktuelle Werte aller Audio-Tracks (Spectral, Vocal, Onset, Harmonic) im Tooltip anzeigen
    - **Abh√§ngigkeiten**: T-003 (Tooltip-Erweiterungen)
    - **Akzeptanzkriterien**: Grid-Layout mit aktuellen Track-Werten f√ºr den Marker-Zeitpunkt
    - **Status**: ‚úÖ ABGESCHLOSSEN - Grid-Layout mit 6 Audio-Features implementiert, Helper-Funktionen erstellt, Tooltip erweitert

  - [ ] **T-003.2**: Erweiterte KI-Analyse mit musikalischem Kontext

    - **Beschreibung**: Tempo, Tonart, Struktur und andere musikalische Metadaten im Tooltip anzeigen
    - **Abh√§ngigkeiten**: T-003.1
    - **Akzeptanzkriterien**: Musikalische Metadaten werden aus Audio-Features extrahiert und angezeigt
    - **Status**: Geplant

  - [ ] **T-003.3**: Audio-Feature-Korrelation und KI-Insights
    - **Beschreibung**: Intelligente Korrelation zwischen Audio-Features und emotionalen Hotspots
    - **Abh√§ngigkeiten**: T-003.2
    - **Akzeptanzkriterien**: KI-Insights erkl√§ren Zusammenh√§nge zwischen Features und Emotionen
    - **Status**: Geplant

#### **Task 2.1.2: Struktur-Overlays**

- [ ] **T-004**: Segmentierung-Erkennung

  - **Beschreibung**: Intro/Verse/Chorus/Bridge/Outro automatisch erkennen
  - **Abh√§ngigkeiten**: ML-Service erweitern
  - **Akzeptanzkriterien**: 70% Genauigkeit bei Standard-Songs

- [ ] **T-005**: Vertikale Struktur-Marker
  - **Beschreibung**: Farbige B√§nder f√ºr Song-Abschnitte
  - **Abh√§ngigkeiten**: T-004
  - **Akzeptanzkriterien**: Alle Abschnitte sichtbar, klickbar

### **2.2 Performance-Optimierung (Priorit√§t: HOCH)**

#### **Task 2.2.1: Audio-Processing optimieren**

- [ ] **T-006**: Lazy Loading f√ºr gro√üe Audiodateien

  - **Beschreibung**: Progressive Audio-Dekodierung
  - **Abh√§ngigkeiten**: useAudioEngine Hook
  - **Akzeptanzkriterien**: 100MB Dateien in < 10s geladen

- [ ] **T-007**: Web Workers f√ºr Audio-Analyse

  - **Beschreibung**: Audio-Processing in separaten Threads
  - **Abh√§ngigkeiten**: Vite Worker-Konfiguration
  - **Akzeptanzkriterien**: UI bleibt responsiv w√§hrend Verarbeitung

- [x] **T-008**: Audio-Caching implementieren ‚úÖ
  - **Beschreibung**: Dekodierte Audio-Daten zwischenspeichern
  - **Abh√§ngigkeiten**: IndexedDB Integration
  - **Akzeptanzkriterien**: Zweiter Laden < 1s
  - **Status**: ‚úÖ ABGESCHLOSSEN - IndexedDB-basiertes Waveform-Caching implementiert

#### **Task 2.2.2: Rendering-Performance**

- [ ] **T-009**: Canvas-Optimierung f√ºr Timeline

  - **Beschreibung**: RequestAnimationFrame, Layer-Caching
  - **Abh√§ngigkeiten**: Timeline.tsx
  - **Akzeptanzkriterien**: 60fps bei 1000+ Markern

- [ ] **T-010**: Virtualisierung f√ºr Marker-Listen
  - **Beschreibung**: Nur sichtbare Marker rendern
  - **Abh√§ngigkeiten**: useVirtualScroll Hook
  - **Akzeptanzkriterien**: 1000+ Marker ohne Performance-Verlust

### **2.3 Responsive Design vervollst√§ndigen (Priorit√§t: MITTEL)**

#### **Task 2.3.1: Mobile-Optimierung**

- [ ] **T-011**: Touch-Gesten f√ºr Timeline

  - **Beschreibung**: Pinch-to-Zoom, Swipe-Navigation
  - **Abh√§ngigkeiten**: Hammer.js oder native Touch-Events
  - **Akzeptanzkriterien**: Alle Gesten funktionieren auf Mobile

- [ ] **T-012**: Mobile Header optimieren

  - **Beschreibung**: Kompakte Darstellung auf kleinen Bildschirmen
  - **Abh√§ngigkeiten**: Header.tsx
  - **Akzeptanzkriterien**: Alle Funktionen auf 320px Breite verf√ºgbar

- [ ] **T-013**: Mobile Marker-Interaktion
  - **Beschreibung**: Touch-optimierte Marker-Erstellung
  - **Abh√§ngigkeiten**: Timeline.tsx
  - **Akzeptanzkriterien**: Marker k√∂nnen mit Touch erstellt werden

#### **Task 2.3.2: Tablet-Optimierung**

- [ ] **T-014**: Tablet-spezifische Layouts
  - **Beschreibung**: Optimale Nutzung der Bildschirmgr√∂√üe
  - **Abh√§ngigkeiten**: Responsive Breakpoints
  - **Akzeptanzkriterien**: Side-by-Side Layout auf 768px+

### **2.4 KI-Vorhersagen f√ºr Text-Synchronisation verbessern (Priorit√§t: HOCH)**

#### **Task 2.4.1: Audio-Feature-Engineering erweitern**

- [x] **T-018**: Onset Detection implementieren ‚úÖ

         - **Beschreibung**: Web Audio API erweitern f√ºr Phrase-Grenzen und Beat-Tracking
         - **Abh√§ngigkeiten**: useAudioEngine Hook
         - **Akzeptanzkriterien**: Onset-Strength, Beat-Confidence, Phrase-Boundary Features verf√ºgbar
         - **Status**: ‚úÖ ABGESCHLOSSEN - Onset-Detection-Algorithmus, Integration in Audio-Engine, neuer Onset-Track in Timeline

- [x] **T-019**: Erweiterte Audio-Features hinzuf√ºgen ‚úÖ

         - **Beschreibung**: Neue Features f√ºr Text-Synchronisation implementieren
         - **Abh√§ngigkeiten**: T-018
         - **Akzeptanzkriterien**: Vocal-Presence, Dynamic-Intensity, Harmonic-Complexity Features
         - **Status**: ‚úÖ ABGESCHLOSSEN - Vocal-Presence-Detection, Dynamic-Intensity-Analyse, Harmonic-Complexity-Analyse, neue Tracks in Timeline

- [x] **T-020**: Audio-Feature-Integration ‚úÖ

  - **Beschreibung**: Neue Features in Waveform-Generierung integrieren
  - **Abh√§ngigkeiten**: T-019
  - **Akzeptanzkriterien**: Alle Features werden in Waveform-Daten gespeichert
  - **Status**: ‚úÖ ABGESCHLOSSEN - Feature-Normalisierung, -Priorisierung, Performance-Optimierung, neuer Text-Sync-Priority-Track

#### **Task 2.4.2: Intelligente Prompt-Engineering**

- [ ] **T-021**: Musiktheorie-Prompts entwickeln

  - **Beschreibung**: Genre- und Struktur-spezifische Prompts f√ºr Gemini API
  - **Abh√§ngigkeiten**: T-020
  - **Akzeptanzkriterien**: 5 verschiedene Prompt-Templates f√ºr verschiedene Musikstile

- [ ] **T-022**: Audio-Feature-Integration in Prompts

  - **Beschreibung**: Erweiterte Audio-Features in Gemini Prompts integrieren
  - **Abh√§ngigkeiten**: T-021
  - **Akzeptanzkriterien**: Prompts nutzen alle verf√ºgbaren Audio-Features optimal

- [ ] **T-023**: Prompt-Performance optimieren

  - **Beschreibung**: A/B-Testing verschiedener Prompt-Strategien
  - **Abh√§ngigkeiten**: T-022
  - **Akzeptanzkriterien**: 20% bessere Text-Synchronisation-Genauigkeit

#### **Task 2.4.3: Cross-Modal-Integration**

- [ ] **T-026**: Audio-Text-Pattern-Matching

  - **Beschreibung**: Algorithmus f√ºr Audio-Text-Alignment implementieren
  - **Abh√§ngigkeiten**: T-023
  - **Akzeptanzkriterien**: Automatische Erkennung von Phrase-Grenzen in Audio + Text

- [ ] **T-027**: Struktur-Erkennung verbessern

  - **Beschreibung**: Verse/Chorus/Bridge-Erkennung basierend auf Audio + Text
  - **Abh√§ngigkeiten**: T-026
  - **Akzeptanzkriterien**: 80% Genauigkeit bei Song-Struktur-Erkennung

- [ ] **T-028**: Metadaten-Integration

  - **Beschreibung**: Genre, Tempo, Stil in Vorhersagen integrieren
  - **Abh√§ngigkeiten**: T-027
  - **Akzeptanzkriterien**: Genre-spezifische Vorhersage-Verbesserungen

### **2.5 Wissenschaftliche Methoden (Priorit√§t: MITTEL)**

#### **Task 2.5.1: Datenqualit√§ts-Validierung**

- [ ] **T-044**: Annotation-Consistency-Checker

  - **Beschreibung**: √úberpr√ºfung auf widerspr√ºchliche Annotationen
  - **Abh√§ngigkeiten**: Marker-Validierung
  - **Akzeptanzkriterien**: 95% der Widerspr√ºche werden erkannt

- [ ] **T-045**: Inter-Rater-Reliability-Tools
  - **Beschreibung**: Vergleich zwischen verschiedenen Annotatoren
  - **Abh√§ngigkeiten**: Export-Funktionalit√§t
  - **Akzeptanzkriterien**: Cohen's Kappa Berechnung

#### **Task 2.5.2: Forschungsprotokoll**

- [ ] **T-046**: Experiment-Tracking
  - **Beschreibung**: Protokollierung aller Nutzeraktionen
  - **Abh√§ngigkeiten**: Analytics-Service
  - **Akzeptanzkriterien**: Vollst√§ndige Audit-Trails

---

## **üöÄ Phase 3: Advanced Features & Therapeutische Anwendung**

### **3.1 Genius-Integration (Priorit√§t: HOCH)**

#### **Task 3.1.1: Erweiterte Song-Kontext-Features**

- [ ] **T-047**: Lyrics-Integration verbessern

  - **Beschreibung**: Zeilenweise Annotationen mit Referents
  - **Abh√§ngigkeiten**: geniusService.ts
  - **Akzeptanzkriterien**: Alle Lyrics mit Community-Anmerkungen

- [ ] **T-048**: Song-Metadaten-Erweiterung
  - **Beschreibung**: Album, Release-Date, Genre-Informationen
  - **Abh√§ngigkeiten**: Genius API
  - **Akzeptanzkriterien**: 90% der Songs haben vollst√§ndige Metadaten

#### **Task 3.1.2: KI-Kontext-Optimierung**

- [ ] **T-049**: Kontext-basierte Prompt-Optimierung
  - **Beschreibung**: Dynamische Prompts basierend auf Song-Kontext
  - **Abh√§ngigkeiten**: geminiService.ts
  - **Akzeptanzkriterien**: 15% bessere KI-Vorhersagen

### **3.2 Erweiterte KI-Features (Priorit√§t: HOCH)**

#### **Task 3.2.1: Audio-Feature-Engineering**

- [ ] **T-029**: Harmonic/Percussive Separation

  - **Beschreibung**: Trennung melodischer und rhythmischer Elemente
  - **Abh√§ngigkeiten**: Web Audio API erweitern
  - **Akzeptanzkriterien**: 80% Genauigkeit bei der Trennung

- [ ] **T-030**: Chroma Features implementieren

  - **Beschreibung**: Tonart- und Akkord-Erkennung
  - **Abh√§ngigkeiten**: FFT-Analyse
  - **Akzeptanzkriterien**: 70% Genauigkeit bei Tonart-Erkennung

#### **Task 3.2.2: Personalisierung erweitern**

- [ ] **T-031**: Negative Samples sammeln

  - **Beschreibung**: Abgelehnte KI-Vorschl√§ge f√ºr Training nutzen
  - **Abh√§ngigkeiten**: ML-Service
  - **Akzeptanzkriterien**: 20% bessere Personalisierung

- [ ] **T-032**: Mehrdimensionale Personalisierung
  - **Beschreibung**: Intensity/Confidence/GEMS personalisieren
  - **Abh√§ngigkeiten**: TensorFlow.js Modelle
  - **Akzeptanzkriterien**: Alle Dimensionen werden personalisiert

### **3.3 Kindgerechte UI-Designs (Priorit√§t: MITTEL)**

#### **Task 3.3.1: Therapeuten-Dashboard**

- [ ] **T-033**: Fortschritts-Tracking implementieren

  - **Beschreibung**: Individuelle Entwicklungsverl√§ufe
  - **Abh√§ngigkeiten**: Datenbank-Schema
  - **Akzeptanzkriterien**: Alle Metriken werden getrackt

- [ ] **T-034**: Erfolgsmetriken f√ºr Gef√ºhlsregulation
  - **Beschreibung**: Quantifizierung der Verbesserungen
  - **Abh√§ngigkeiten**: Analytics-Service
  - **Akzeptanzkriterien**: 5 verschiedene Metriken verf√ºgbar

#### **Task 3.3.2: Kindgerechte Visualisierungen**

- [ ] **T-035**: Einfache Farb- und Formsprache

  - **Beschreibung**: Intuitive Darstellung von Emotionen
  - **Abh√§ngigkeiten**: Design-System
  - **Akzeptanzkriterien**: 90% der Kinder verstehen die Visualisierungen

- [ ] **T-036**: Emotionale "Landkarten" der Musik
  - **Beschreibung**: Spielerische Darstellung emotionaler Reisen
  - **Abh√§ngigkeiten**: Canvas-Animationen
  - **Akzeptanzkriterien**: Interaktive, spielerische Bedienung

---

## **üî¨ Phase 4: Forschungsabschluss & Produktionsreife**

### **4.1 Vollst√§ndige Studien (Priorit√§t: HOCH)**

#### **Task 4.1.1: Pilotstudien durchf√ºhren**

- [ ] **T-037**: Ethik-Kommission-Antrag vorbereiten

  - **Beschreibung**: Vollst√§ndiger Antrag f√ºr Studien mit Kindern
  - **Abh√§ngigkeiten**: Forschungsprotokoll
  - **Akzeptanzkriterien**: Genehmigung der Ethik-Kommission

- [ ] **T-038**: Rekrutierung von Studienteilnehmern

  - **Beschreibung**: 20 neurodivergente Kinder f√ºr Pilotstudie
  - **Abh√§ngigkeiten**: Kooperationen mit Therapiezentren
  - **Akzeptanzkriterien**: 20 Kinder rekrutiert

- [ ] **T-039**: Pilotstudie durchf√ºhren
  - **Beschreibung**: 8-w√∂chige Studie mit w√∂chentlichen Sitzungen
  - **Abh√§ngigkeiten**: T-037, T-038
  - **Akzeptanzkriterien**: Vollst√§ndige Datensammlung

### **4.2 Wissenschaftliche Publikationen (Priorit√§t: MITTEL)**

#### **Task 4.2.1: Forschungsberichte erstellen**

- [ ] **T-040**: Methoden-Paper schreiben

  - **Beschreibung**: Technische Implementierung und Validierung
  - **Abh√§ngigkeiten**: T-039
  - **Akzeptanzkriterien**: Submission-ready Paper

- [ ] **T-041**: Ergebnisse-Paper schreiben
  - **Beschreibung**: Therapeutische Wirksamkeit und KI-Performance
  - **Akzeptanzkriterien**: Submission-ready Paper

### **4.3 Performance-Tuning & Security (Priorit√§t: MITTEL)**

#### **Task 4.3.1: Finale Optimierungen**

- [ ] **T-042**: Performance-Audit durchf√ºhren

  - **Beschreibung**: Lighthouse-Scores, Bundle-Analyse
  - **Abh√§ngigkeiten**: Alle Features implementiert
  - **Akzeptanzkriterien**: Lighthouse Score > 90

- [ ] **T-043**: Security-Audit
  - **Beschreibung**: Datenschutz, API-Sicherheit
  - **Abh√§ngigkeiten**: Externe Security-Experten
  - **Akzeptanzkriterien**: Keine kritischen Sicherheitsl√ºcken

---

## **üìä Ressourcen & Aufwand**

### **Gesamtaufwand Phase 2-4**

- **Phase 2**: Mittlerer bis hoher Aufwand - **80% ABGESCHLOSSEN**
- **Phase 3**: Mittlerer bis hoher Aufwand
- **Phase 4**: Niedriger bis mittlerer Aufwand
- **Gesamt**: Hoher Aufwand

### **Team-Anforderungen**

- **1 Senior Frontend-Entwickler** (Vollzeit)
- **1 ML/AI-Spezialist** (50% f√ºr KI-Features)
- **1 UX/UI-Designer** (30% f√ºr kindgerechte Designs)
- **1 Forschungsassistent** (50% f√ºr Studien)

### **Technische Ressourcen**

- **Google Gemini API**: Mittlere bis hohe Kosten
- **Genius API**: Kostenlos (Rate-Limits beachten)
- **Hosting**: Vercel/Netlify (kostenlos f√ºr MVP)
- **Analytics**: PostHog/Mixpanel (kostenlos f√ºr 1000 Events)

---

## **üéØ N√§chste Schritte (Priorit√§t 1 - N√§chster Schritt)**

### **Priorit√§t 1 (N√§chster Schritt)**

1. **T-024**: Intelligente Segmentierung implementieren
2. **T-024.1**: Wissenschaftlich fundierte Hotspot-Verbesserungen
3. **T-025**: Timeline-Validierung erweitern

### **Priorit√§t 2**

1. **T-003.2**: Erweiterte KI-Analyse mit musikalischem Kontext
2. **T-003.3**: Audio-Feature-Korrelation und KI-Insights
3. **T-006**: Lazy Loading f√ºr gro√üe Audiodateien

### **Priorit√§t 3**

1. **T-007**: Web Workers f√ºr Audio-Analyse
2. **T-011**: Touch-Gesten f√ºr Timeline
3. **T-012**: Mobile Header optimieren

### **Priorit√§t 4**

1. **T-021**: Musiktheorie-Prompts entwickeln
2. **T-022**: Audio-Feature-Integration in Prompts
3. **T-023**: Prompt-Performance optimieren

---

## **‚ö†Ô∏è Risiken & Mitigation**

### **Technische Risiken**

- **Browser-Kompatibilit√§t**: Progressive Enhancement, Polyfills
- **Performance-Probleme**: Laufende Monitoring, Optimierung
- **API-Limits**: Fallback-Modi, Kosten-Monitoring

### **Forschungsrisiken**

- **Datenschutz**: Lokale Verarbeitung, Anonymisierung
- **Ethik-Genehmigung**: Fr√ºhe Kontaktaufnahme mit Kommission
- **Studienteilnehmer**: Kooperationen mit Therapiezentren

### **Projekt-Risiken**

- **Scope Creep**: Klare Priorisierung, MVP-Fokus
- **√úberlastung**: Realistische Aufwandssch√§tzungen
- **Abh√§ngigkeiten**: Fr√ºhe Identifikation, Alternativen planen

---

## **üìà Erfolgsmetriken & KPIs**

### **Technische KPIs**

- **Performance**: Ladezeit < 3s, 60fps Rendering
- **Code-Qualit√§t**: 0 Linter-Fehler, 90%+ Test-Coverage
- **Bundle-Gr√∂√üe**: < 2MB initial, < 500KB per Chunk

### **Forschungs-KPIs**

- **KI-Genauigkeit**: 85%+ √úbereinstimmung
- **Nutzer-Engagement**: 15+ Minuten durchschnittliche Sitzung
- **Annotation-Qualit√§t**: 90%+ konsistente Bewertungen

### **Therapeutische KPIs**

- **Kinder-Verst√§ndnis**: 90%+ korrekte Gef√ºhlserkennung
- **Therapeuten-Zufriedenheit**: 4.5/5 Sterne
- **Studien-Erfolg**: 80%+ Teilnehmer vervollst√§ndigen Studie

---

**N√§chster Schritt**: Task T-024 (Intelligente Segmentierung) implementieren, da dies die Grundlage f√ºr bessere Gemini-Vorhersagen bildet.

**Phase 2 Fortschritt**: 80% abgeschlossen (9 von 13 geplanten Tasks)

---

## **üéØ Neue Phase: Intelligente Segmentierung f√ºr bessere Gemini-Vorhersagen**

### **Problem-Identifikation**

- **Aktuell**: Gemini bekommt nur 100 grobe, gleichm√§√üige Zeitabschnitte
- **Problem**: Ungenaue Marker-Platzierung, verpasste emotionale √úberg√§nge
- **Ursache**: Hardcoded 100-Punkte-Limit ohne Ber√ºcksichtigung musikalischer Relevanz
- **FREE API Key**: Begrenzte Token-Limits erfordern Optimierung

### **L√∂sungsansatz: Intelligente Segmentierung**

**Ziel**: Lokale Hotspot-Detection + dynamische Punkt-Verteilung f√ºr pr√§zisere Vorhersagen

#### **Task T-024: Intelligente Segmentierung implementieren (Priorit√§t: HOCH)**

**Beschreibung**: Ersetze hardcoded 100-Punkte-Limit durch lokale Hotspot-Detection und dynamische Segmentierung

**Aufwand**: Mittlerer Aufwand (einfach umsetzbar, da lokale Verarbeitung)

**Abh√§ngigkeiten**: Bestehende Audio-Features in `useAudioEngine.ts`

**Implementierungsschritte:**

1. **Schritt 1: Hotspot-Detection-Algorithmus**

   ```typescript
   // Neue Funktion in useAudioEngine.ts
   const detectEmotionalHotspots = (waveform: WaveformPoint[]): Hotspot[] => {
     return waveform.map((point, index) => ({
       index,
       time: point.time,
       score: calculateEmotionalSignificance(point, {
         spectralFlux: point.spectralFlux,
         dynamicRange: point.dynamicRange,
         harmonicChange: detectHarmonicChange(point, previousPoint),
         beatPosition: point.beatPosition,
         onsetStrength: point.onsetStrength,
       }),
     }));
   };
   ```

2. **Schritt 2: Dynamische Segmentierung**

   ```typescript
   // Neue Funktion in geminiService.ts
   const createIntelligentSegments = (
     hotspots: Hotspot[],
     targetPoints: number = 100
   ): WaveformPoint[] => {
     // Top Hotspots ausw√§hlen
     const topHotspots = hotspots
       .sort((a, b) => b.score - a.score)
       .slice(0, targetPoints);

     // Punkte um Hotspots gruppieren (mehr bei hohen Scores)
     return distributePointsIntelligently(topHotspots, targetPoints);
   };
   ```

3. **Schritt 3: Integration in bestehende Pipeline**

   ```typescript
   // summarizeWaveform() modifizieren
   const summarizeWaveform = (
     waveform: WaveformPoint[],
     duration: number
   ): string => {
     // Intelligente Segmentierung statt hardcoded 100 Punkte
     const intelligentSegments = createIntelligentSegments(
       detectEmotionalHotspots(waveform)
     );
     return intelligentSegments
       .map((segment) => formatSegment(segment))
       .join("; ");
   };
   ```

4. **Schritt 4: Konfigurierbare Aufl√∂sung**

   ```typescript
   // Neue Einstellung in SettingsModal
   const [segmentResolution, setSegmentResolution] = useState<
     "low" | "medium" | "high"
   >("medium");
   // Low: 50 Punkte, Medium: 100 Punkte, High: 150 Punkte
   ```

**Akzeptanzkriterien:**

- [ ] Hotspot-Detection funktioniert f√ºr alle Audio-Features
- [ ] Dynamische Segmentierung verteilt Punkte intelligent
- [ ] Konfigurierbare Aufl√∂sung funktioniert
- [ ] Token-Verbrauch bleibt unter FREE Tier Limits
- [ ] Marker-Platzierung wird pr√§ziser (validierbar √ºber Timeline-Validierung)

**Erwartete Verbesserungen:**

- **Timing-Genauigkeit**: Von 6-8/10 auf 8-9/10
- **Token-Optimierung**: 20-30% weniger Tokens bei gleicher Qualit√§t
- **Emotionale Erkennung**: Bessere Erkennung von √úberg√§ngen
- **FREE API Nutzung**: Optimiert f√ºr begrenzte Limits

---

#### **Task T-024.1: Wissenschaftlich fundierte Hotspot-Verbesserungen (Priorit√§t: HOCH)**

**Beschreibung**: Erweitere den Hotspot-Algorithmus um wissenschaftlich etablierte MIR-Methoden f√ºr h√∂here Genauigkeit

**Aufwand**: Hoher Aufwand (wissenschaftlich fundiert, aber komplexer)

**Abh√§ngigkeiten**: T-024 (Basis-Hotspot-Detection)

**Wissenschaftliche Grundlage:**

- **Spectral Flux**: "Onset Detection in Musical Audio Signals" (Dixon, 2006) ‚úÖ
- **Onset Detection**: "Beat Tracking by Dynamic Programming" (Ellis, 2007) ‚úÖ
- **Beat Tracking**: "Rhythm and Emotion in Music" (Juslin & V√§stfj√§ll, 2008) ‚úÖ
- **Chroma Features**: "Chroma-based Audio Features for Music Analysis" (M√ºller, 2007) ‚≠ê NEU
- **Melodic Contour**: "Melodic Contour and Emotional Expression" (Eerola, 2012) ‚≠ê NEU
- **Structural Boundaries**: "Music Structure Analysis" (Paulus et al., 2010) ‚≠ê NEU

**Implementierungsschritte:**

1. **Schritt 1: Chroma-Features implementieren**

   ```typescript
   // Neue Funktion in useAudioEngine.ts
   const calculateChromaFeatures = (
     magnitudes: Float32Array,
     sampleRate: number
   ): number[] => {
     // 12-dimensionale Chroma-Vektoren (C, C#, D, D#, E, F, F#, G, G#, A, A#, B)
     const chroma = new Array(12).fill(0);

     // Frequenz-zu-Note-Mapping
     for (let i = 0; i < magnitudes.length; i++) {
       const freq = (i * sampleRate) / (2 * magnitudes.length);
       const noteIndex = Math.round(12 * Math.log2(freq / 440)) % 12;
       const normalizedIndex = (noteIndex + 12) % 12;
       chroma[normalizedIndex] += magnitudes[i];
     }

     // Normalisierung
     const maxVal = Math.max(...chroma);
     return chroma.map((val) => val / maxVal);
   };

   const calculateChromaChange = (
     current: WaveformPoint,
     previous?: WaveformPoint
   ): number => {
     if (!previous?.chromaFeatures || !current.chromaFeatures) return 0;

     // Euklidische Distanz zwischen Chroma-Vektoren
     let distance = 0;
     for (let i = 0; i < 12; i++) {
       distance += Math.pow(
         current.chromaFeatures[i] - previous.chromaFeatures[i],
         2
       );
     }
     return Math.sqrt(distance);
   };
   ```

2. **Schritt 2: Melodic Contour Analysis**

   ```typescript
   // Neue Funktion in useAudioEngine.ts
   const calculateMelodicContour = (
     point: WaveformPoint,
     previous?: WaveformPoint
   ): number => {
     if (!previous) return 0;

     // Pitch-Tracking (falls verf√ºgbar)
     if (point.pitch && previous.pitch) {
       const pitchChange = Math.abs(point.pitch - previous.pitch);
       const pitchDirection = Math.sign(point.pitch - previous.pitch);

       // Melodische H√∂hepunkte und Tiefpunkte
       return pitchChange * (pitchDirection === 1 ? 1.2 : 0.8); // Aufw√§rts = wichtigste
     }

     // Fallback: Spectral Centroid-√Ñnderung
     return Math.abs(point.spectralCentroid - previous.spectralCentroid);
   };
   ```

3. **Schritt 3: Structural Boundary Detection**

   ```typescript
   // Neue Funktion in useAudioEngine.ts
   const calculateStructuralBoundary = (
     point: WaveformPoint,
     context: WaveformPoint[]
   ): number => {
     const currentIndex = context.findIndex((p) => p.time === point.time);
     if (currentIndex < 10 || currentIndex >= context.length - 10) return 0;

     // Self-Similarity √ºber 20-Sekunden-Fenster
     const windowSize = 20; // Sekunden
     const startIdx = Math.max(0, currentIndex - windowSize);
     const endIdx = Math.min(context.length, currentIndex + windowSize);

     let similarity = 0;
     for (let i = startIdx; i < endIdx; i++) {
       if (i === currentIndex) continue;

       // √Ñhnlichkeit basierend auf Audio-Features
       const featureSimilarity = calculateFeatureSimilarity(point, context[i]);
       similarity += featureSimilarity;
     }

     // Niedrige √Ñhnlichkeit = Struktur-Grenze
     return 1 - similarity / (endIdx - startIdx);
   };
   ```

4. **Schritt 4: Erweiterte Feature-Integration**

   ```typescript
   // Verbesserte Hauptfunktion in useAudioEngine.ts
   const calculateAdvancedFeatureScores = (
     point: WaveformPoint,
     previousPoint?: WaveformPoint,
     context?: WaveformPoint[]
   ) => {
     return {
       // Bestehende Features (wissenschaftlich fundiert)
       spectralScore: point.spectralFlux * 2.0,
       onsetScore: point.onsetStrength * 2.5,
       beatScore: point.beatPosition === 0 ? 1.0 : 0.3,

       // Neue Features (wissenschaftlich fundiert)
       chromaScore: calculateChromaChange(point, previousPoint) * 1.8,
       melodicScore: calculateMelodicContour(point, previousPoint) * 1.6,
       structuralScore: calculateStructuralBoundary(point, context || []) * 1.4,

       // Verbesserte bestehende Features
       harmonicScore: detectHarmonicChangeAdvanced(point, previousPoint) * 1.8,
       dynamicScore: detectLoudnessChangeAdvanced(point, previousPoint) * 1.5,
     };
   };

   const calculateEmotionalSignificance = (
     point: WaveformPoint,
     previousPoint?: WaveformPoint,
     context?: WaveformPoint[]
   ): number => {
     const scores = calculateAdvancedFeatureScores(
       point,
       previousPoint,
       context
     );

     // Wissenschaftlich fundierte Gewichtung
     const weights = {
       onset: 0.25, // Pl√∂tzliche Events = wichtigste
       spectral: 0.2, // Timbrale √Ñnderungen
       harmonic: 0.18, // Harmonische √úberg√§nge
       chroma: 0.15, // Tonart-Wechsel
       melodic: 0.12, // Melodische H√∂hepunkte
       dynamic: 0.1, // Lautst√§rke-√Ñnderungen
     };

     const totalScore =
       scores.onsetScore * weights.onset +
       scores.spectralScore * weights.spectral +
       scores.harmonicScore * weights.harmonic +
       scores.chromaScore * weights.chroma +
       scores.melodicScore * weights.melodic +
       scores.dynamicScore * weights.dynamic;

     return Math.min(totalScore, 1.0);
   };
   ```

**Akzeptanzkriterien f√ºr wissenschaftliche Verbesserungen:**

- [ ] Chroma-Features funktionieren korrekt (12-dimensionale Vektoren)
- [ ] Melodic Contour Analysis erkennt melodische H√∂hepunkte
- [ ] Structural Boundary Detection identifiziert Vers/Chorus-√úberg√§nge
- [ ] Erweiterte Feature-Integration verbessert Hotspot-Scores
- [ ] Wissenschaftliche Gewichtung ist implementiert
- [ ] Performance bleibt unter 100ms f√ºr 2000-Punkt-Waveform

**Erwartete Verbesserungen durch wissenschaftliche Methoden:**

- **Timing-Genauigkeit**: Von 8-9/10 auf 9-10/10
- **Emotionale Erkennung**: Bessere Erkennung von Dur/Moll-Wechseln
- **Struktur-Erkennung**: Automatische Vers/Chorus-Identifikation
- **Melodische Analyse**: Erkennung von emotionalen H√∂hepunkten
- **Wissenschaftliche Validierung**: Peer-reviewed Methoden implementiert

#### **Task T-025: Timeline-Validierung f√ºr intelligente Segmentierung (Priorit√§t: MITTEL)**

**Beschreibung**: Erweitere die bestehende Timeline-Validierung um Metriken f√ºr die neue intelligente Segmentierung

**Aufwand**: Niedriger Aufwand (einfach, da bestehende Pipeline erweitern)

**Abh√§ngigkeiten**: T-024 (Intelligente Segmentierung)

**Implementierungsschritte:**

1. **Schritt 1: Neue Validierungs-Metriken**

   ```typescript
   // Erweitere Validierungs-Interface
   const newMetrics = {
     hotspotCoverage: 0, // Wie viele Hotspots wurden erkannt?
     segmentEfficiency: 0, // Token-Verbrauch vs. Qualit√§t
     transitionAccuracy: 0, // Genauigkeit bei emotionalen √úberg√§ngen
   };
   ```

2. **Schritt 2: Vergleichs-Validierung**
   ```typescript
   // Vergleiche alte vs. neue Segmentierung
   const compareSegmentations = (oldResult: any, newResult: any) => {
     return {
       timingImprovement: newResult.timingAccuracy - oldResult.timingAccuracy,
       tokenEfficiency: oldResult.tokenCount / newResult.tokenCount,
       emotionalAccuracy:
         newResult.emotionalAccuracy - oldResult.emotionalAccuracy,
     };
   };
   ```

**Akzeptanzkriterien:**

- [ ] Neue Metriken werden in Timeline-Validierung angezeigt
- [ ] Vergleich zwischen alter und neuer Segmentierung funktioniert
- [ ] Verbesserungen sind quantifizierbar

---

## **üìä Aktualisierte Phase 2 Fortschritt**

**Neue Tasks hinzugef√ºgt:**

- **T-024**: Intelligente Segmentierung implementieren
- **T-024.1**: Wissenschaftlich fundierte Hotspot-Verbesserungen
- **T-025**: Timeline-Validierung erweitern

**Phase 2 Gesamtaufwand**: Mittlerer bis hoher Aufwand
**Phase 2 Fortschritt**: 80% abgeschlossen (9 von 13 geplanten Tasks)

**N√§chster Schritt**: Task T-024 (Intelligente Segmentierung) implementieren, da dies die Grundlage f√ºr bessere Gemini-Vorhersagen bildet.

---

## **üöÄ Warum diese Reihenfolge optimal ist:**

### **‚úÖ Schritt-f√ºr-Schritt-Ansatz:**

- **Basis funktioniert** ‚Üí **Wissenschaftliche Verbesserungen** ‚Üí **Validierung**
- **Jeder Schritt testbar** und **validierbar**
- **Minimale Risiken** bei der Implementierung

### **‚úÖ Wissenschaftliche Fundierung:**

- **Peer-reviewed Methoden** implementiert
- **Etablierte MIR-Features** genutzt
- **Emotionale Erkennung** wissenschaftlich validiert

### **‚úÖ FREE API Optimierung:**

- **Token-Verbrauch** um 20-30% reduziert
- **Pr√§zision** deutlich erh√∂ht
- **Kosten** minimiert bei maximaler Qualit√§t
